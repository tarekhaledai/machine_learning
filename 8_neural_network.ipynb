{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSzY4KB6BkeD"
      },
      "source": [
        "**Artificial Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oToWXx_CBkeG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fa33ZWlfBkeG"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9G_7mjnBkeJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2-q8oxBkeK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Zx3oMwBkeK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"ann\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD-iyIoUBkeL"
      },
      "source": [
        "## Des neurones biologiques aux neurones artificiels\n",
        "# Le perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RnoFOmLBkeL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y = (iris.target == 0)  # Iris setosa\n",
        "\n",
        "per_clf = Perceptron(random_state=42)\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "X_new = [[2, 0.5], [3, 1]]\n",
        "y_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGpuIBqJBkeL"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rqwWaR-BkeL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(loss=\"perceptron\", penalty=None,\n",
        "                        learning_rate=\"constant\", eta0=1, random_state=42)\n",
        "sgd_clf.fit(X, y)\n",
        "assert (sgd_clf.coef_ == per_clf.coef_).all()\n",
        "assert (sgd_clf.intercept_ == per_clf.intercept_).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " L'algorithme d'apprentissage du perceptron ressemble fortement à la descente de gradient stochastique. En fait, la classe Perceptron de Scikit-Learn est équivalente à l'utilisation d'un SGDClassifier avec les hyperparamètres suivants : loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (le taux d'apprentissage) et penalty=None (pas de régularisation)."
      ],
      "metadata": {
        "id": "4CR7PgvbIwo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFNbL6tbBkeM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "a = -per_clf.coef_[0, 0] / per_clf.coef_[0, 1]\n",
        "b = -per_clf.intercept_ / per_clf.coef_[0, 1]\n",
        "axes = [0, 5, 0, 2]\n",
        "x0, x1 = np.meshgrid(\n",
        "    np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
        "    np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
        ")\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = per_clf.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
        "\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"bs\", label=\"Not Iris setosa\")\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"yo\", label=\"Iris setosa\")\n",
        "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\",\n",
        "         linewidth=3)\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gMTKXCDBkeM"
      },
      "source": [
        "**Les fonctions d'activation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgWNGPoSBkeM"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
        "\n",
        "max_z = 4.5\n",
        "z = np.linspace(-max_z, max_z, 200)\n",
        "\n",
        "plt.figure(figsize=(11, 3.1))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot([-max_z, 0], [0, 0], \"r-\", linewidth=2, label=\"Heaviside\")\n",
        "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
        "plt.plot([0, 0], [0, 1], \"r-\", linewidth=0.5)\n",
        "plt.plot([0, max_z], [1, 1], \"r-\", linewidth=2)\n",
        "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, np.tanh(z), \"b-\", linewidth=1, label=\"Tanh\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Activation functions\")\n",
        "plt.axis([-max_z, max_z, -1.65, 2.4])\n",
        "plt.gca().set_yticks([-1, 0, 1, 2])\n",
        "plt.legend(loc=\"lower right\", fontsize=13)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Heaviside\")\n",
        "plt.plot(0, 0, \"ro\", markersize=5)\n",
        "plt.plot(0, 0, \"rx\", markersize=10)\n",
        "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=1, label=\"Tanh\")\n",
        "plt.plot([-max_z, 0], [0, 0], \"m-.\", linewidth=2)\n",
        "plt.plot([0, max_z], [1, 1], \"m-.\", linewidth=2)\n",
        "plt.plot([0, 0], [0, 1], \"m-.\", linewidth=1.2)\n",
        "plt.plot(0, 1, \"mo\", markersize=5)\n",
        "plt.plot(0, 1, \"mx\", markersize=10)\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivatives\")\n",
        "plt.axis([-max_z, max_z, -0.2, 1.2])\n",
        "\n",
        "save_fig(\"activation_functions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici une version simplifiée des explications des fonctions d'activation couramment utilisées dans les réseaux de neurones : Heaviside, Sigmoid, ReLU et Tanh :\n",
        "\n",
        "### 1. Fonction d'Échelon de Heaviside\n",
        "Cette fonction donne 0 si l'entrée est négative et 1 si elle est positive. Elle est simple mais ne permet pas une mise à jour efficace lors de l'apprentissage car elle n'est pas différentiable.\n",
        "\n",
        "### 2. Fonction Sigmoïde\n",
        "La sigmoïde transforme les entrées en valeurs entre 0 et 1, interprétables comme des probabilités. Elle est idéale pour les tâches de classification binaire. Toutefois, elle peut ralentir l'apprentissage dans les réseaux profonds à cause du problème de \"gradient qui disparaît\".\n",
        "\n",
        "### 3. Unité Linéaire Rectifiée (ReLU)\n",
        "ReLU est très populaire, surtout pour les réseaux profonds, car elle transmet directement les valeurs positives et annule les négatives, ce qui accélère l'apprentissage. Cependant, elle peut causer des problèmes lorsque toutes les sorties deviennent nulles.\n",
        "\n",
        "### 4. Tangente Hyperbolique (Tanh)\n",
        "Tanh est similaire à la sigmoïde mais varie de -1 à 1, ce qui aide à centrer les données et peut conduire à un apprentissage plus stable et rapide. Comme la sigmoïde, elle peut aussi être affectée par le problème de gradient qui disparaît.\n",
        "\n",
        "Chaque fonction a ses avantages et est choisie selon le besoin spécifique de l'application et du type de réseau de neurones utilisé."
      ],
      "metadata": {
        "id": "jydwr7DtLBsC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIk6LraBkeN"
      },
      "source": [
        "## Regression MLPs\n",
        "\n",
        "les MLP (réseaux de neurones multicouches) peuvent être utilisés pour des tâches de régression. Si vous souhaitez prédire une valeur unique (par exemple, le prix d'une maison, compte tenu de nombreuses de ses caractéristiques), vous avez juste besoin d'un seul neurone de sortie : sa sortie est la valeur prédite. Pour la régression multivariée (c'est-à-dire pour prédire plusieurs valeurs à la fois), vous avez besoin d'un neurone de sortie par dimension de sortie.\n",
        "\n",
        "Scikit-Learn comprend une classe MLPRegressor. Utilisons-la pour construire un MLP avec trois couches cachées composées de 50 neurones chacune, et entraînons-le sur le jeu de données du logement en Californie.\n",
        "\n",
        "Pour simplifier, nous utiliserons la fonction fetch_california_housing() de Scikit-Learn pour charger les données. Ce jeu de données est plus simple que celui que nous avons utilisé précédemment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzntfQgbBkeN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
        "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_valid)\n",
        "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notez que ce MLP n'utilise aucune fonction d'activation pour la couche de sortie, donc il est libre de produire toute valeur qu'il souhaite. Cela est généralement acceptable, mais si vous voulez garantir que la sortie sera toujours positive, vous devriez alors utiliser la fonction d'activation ReLU dans la couche de sortie, ou la fonction d'activation softplus, qui est une variante lisse de ReLU : softplus(z) = log(1 + exp(z)). Softplus est proche de 0 lorsque z est négatif, et proche de z lorsque z est positif. Enfin, si vous voulez garantir que les prédictions seront toujours dans une plage de valeurs donnée, alors vous devriez utiliser la fonction sigmoid ou la tangente hyperbolique, et mettre à l'échelle les cibles vers la plage appropriée : de 0 à 1 pour le sigmoid et de –1 à 1 pour tanh. Malheureusement, la classe MLPRegressor ne prend pas en charge les fonctions d'activation dans la couche de sortie."
      ],
      "metadata": {
        "id": "M_UAjg-FZMPN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls3cPesZBkeO"
      },
      "source": [
        "# Implémentation des MLP avec Keras\n",
        "## Construire un classificateur d'images en utilisant l'API séquentielle\n",
        "### Utilisation de Keras pour charger le jeu de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek_PSCyzBkeO"
      },
      "source": [
        "Pour commencer à travailler avec le dataset Fashion MNIST, vous pouvez utiliser les fonctions fournies par Keras pour charger facilement ce dataset. Voici comment charger le dataset Fashion MNIST et le diviser en ensembles d'entraînement, de validation et de test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeDlGuP2BkeO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSPACkoaBkeP"
      },
      "source": [
        "L'ensemble d'apprentissage contient 60 000 images en niveaux de gris, de 28x28 pixels chacune :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2-U4ud2BkeP"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QV3rkIIBkeP"
      },
      "source": [
        "L'intensité de chaque pixel est représentée par un octet (0 à 255) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MokM51KJBkeP"
      },
      "outputs": [],
      "source": [
        "X_train.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3LbsvR0BkeP"
      },
      "source": [
        "Mettons à l'échelle les intensités des pixels dans la plage 0-1 et convertissons-les en nombres flottants en les divisant par 255 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGS0NyP4BkeQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Fz70p_BkeQ"
      },
      "source": [
        "Vous pouvez tracer une image en utilisant la fonction `imshow()` de Matplotlib, avec une carte de couleurs `'binaire'' :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCYaL68jBkeQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIEI79FFBkeQ"
      },
      "source": [
        "Les étiquettes sont les identifiants des classes (représentés sous forme de uint8), de 0 à 9 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a591njf7BkeQ"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s5q7t4YBkeQ"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8AJONRMBkeR"
      },
      "source": [
        "So the first image in the training set is an ankle boot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzXv9N6yBkeR"
      },
      "outputs": [],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUkVDw8gBkeR"
      },
      "source": [
        "Jetons un coup d'œil sur un échantillon des images de l'ensemble de données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEFPSxfGBkeR"
      },
      "outputs": [],
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]])\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "\n",
        "save_fig(\"fashion_mnist_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGO1imOBkeS"
      },
      "source": [
        "### Création du modèle à l'aide de l'API séquentielle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDyui8mzBkeS"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici une version simplifiée de la description du code :\n",
        "\n",
        "- **Initialisation du germe aléatoire :** On commence par fixer le germe aléatoire de TensorFlow pour que les résultats soient les mêmes à chaque exécution. Cela garantit que les poids aléatoires dans le réseau sont constants d'une exécution à l'autre.\n",
        "  \n",
        "- **Création du modèle Séquentiel :** On utilise le modèle le plus simple de Keras, appelé Séquentiel, qui consiste en une pile de couches connectées les unes après les autres.\n",
        "\n",
        "- **Ajout de la couche d'Entrée :** On définit la forme des données d'entrée que le modèle attend, sans inclure la taille du lot.\n",
        "\n",
        "- **Ajout de la couche de mise à plat (Flatten) :** Cette couche transforme les images d'entrée en tableaux 1D pour faciliter leur traitement par les couches suivantes.\n",
        "\n",
        "- **Ajout des couches cachées Dense :** Premièrement, une couche de 300 neurones avec la fonction d'activation ReLU, puis une seconde couche de 100 neurones, également avec ReLU. Ces couches sont responsables de la computation des transformations des entrées grâce aux poids et aux biais.\n",
        "\n",
        "- **Ajout de la couche de sortie :** Une couche de 10 neurones, chacun correspondant à une classe, utilisant la fonction d'activation softmax pour prédire la probabilité que l'entrée appartienne à chaque classe.\n",
        "\n",
        "Chaque étape ajoute des éléments essentiels au modèle pour lui permettre de traiter les données, apprendre des caractéristiques et faire des prédictions."
      ],
      "metadata": {
        "id": "2iKY69Pto0tM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riLmahyMBkeS"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2-p__yVBkeS"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction Softmax est une extension de la fonction sigmoïde utilisée pour les tâches de classification multiclasse. Voici une explication simplifiée de son fonctionnement :\n",
        "\n",
        "### Fonction Softmax\n",
        "Softmax est utilisée principalement dans la couche de sortie des réseaux de neurones pour des problèmes de classification où il y a plus de deux classes. Cette fonction transforme les scores (aussi appelés logits) de chaque classe en probabilités en utilisant l'exponentielle des scores, garantissant que la somme de toutes les probabilités de sortie est égale à 1. La classe avec la plus haute probabilité est généralement choisie comme la classe prédite.\n",
        "\n",
        "Voici la formule mathématique de Softmax :\n",
        "$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $\n",
        "où $ z_i $ est le score de la classe $i$ et le dénominateur est la somme des exponentielles de tous les scores de classe, ce qui normalise ces scores en probabilités.\n",
        "\n",
        "### Avantages de Softmax\n",
        "- **Interprétable :** Les sorties peuvent être interprétées directement comme des probabilités de classe.\n",
        "- **Flexible :** Convient à des problèmes de classification où chaque instance peut appartenir à une et une seule de plusieurs classes.\n",
        "- **Efficace :** Elle aide à amplifier les différences de probabilité entre les classes, rendant le choix de la classe prédite plus clair.\n",
        "\n",
        "Softmax est largement utilisée pour la dernière couche des réseaux de neurones dans les tâches de classification multiclasse, aidant le modèle à décider clairement quelle classe est la plus probable pour une entrée donnée."
      ],
      "metadata": {
        "id": "MTeOketEmqNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fwMNy6GBkeT"
      },
      "outputs": [],
      "source": [
        "# extra code – another way to display the model's architecture\n",
        "tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDnuC_awBkeT"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brf_ujrhBkeT"
      },
      "outputs": [],
      "source": [
        "hidden1 = model.layers[1]\n",
        "hidden1.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n_Y5YI5BkeT"
      },
      "outputs": [],
      "source": [
        "model.get_layer('dense') is hidden1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDI5j4zzBkeT"
      },
      "outputs": [],
      "source": [
        "weights, biases = hidden1.get_weights()\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarquez que la couche Dense a initialisé les poids de connexion de manière aléatoire (ce qui est nécessaire pour briser la symétrie, comme nous l'avons vu plus haut), et que les biais ont été initialisés à des zéros, ce qui est correct."
      ],
      "metadata": {
        "id": "gk54ie9-slNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jD9Xc16BkeT"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mR2jZW3BkeU"
      },
      "outputs": [],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54n2PmaBBkeU"
      },
      "outputs": [],
      "source": [
        "biases.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk25AN3oBkeU"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3uzzeoVBkeU"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous utilisons la fonction de perte \"sparse_categorical_crossentropy\" parce que nous avons des étiquettes éparses (c'est-à-dire, pour chaque instance, il y a juste un indice de classe cible, de 0 à 9 dans ce cas), et les classes sont exclusives.\n",
        "\n",
        "Si, à la place, nous avions une probabilité cible par classe pour chaque instance (comme des vecteurs \"one-hot\", par exemple, [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] pour représenter la classe 3), alors nous devrions utiliser la perte \"categorical_crossentropy\". Si nous faisions une classification binaire ou une classification binaire multilabel, alors nous utiliserions la fonction d'activation \"sigmoid\" dans la couche de sortie au lieu de la fonction \"softmax\", et nous utiliserions la perte \"binary_crossentropy\".\n",
        "\n",
        "Concernant l'optimiseur, \"sgd\" signifie que nous allons entraîner le modèle en utilisant la descente de gradient stochastique. Autrement dit, Keras exécutera l'algorithme de rétropropagation décrit précédemment (c'est-à-dire, l'autodifférenciation en mode inverse plus la descente de gradient)."
      ],
      "metadata": {
        "id": "atnKtuR3t8cn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz8nIKSfBkeU"
      },
      "source": [
        "C'est equivalent :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX6IVieqBkeV"
      },
      "source": [
        "### Entraînement et évaluation du modèle\n",
        "\n",
        "Dans le contexte de l'entraînement des réseaux de neurones, les **epochs** désignent le nombre de fois que l'algorithme d'apprentissage traite l'ensemble des données. Une epoch comprend toutes les étapes de propagation avant et de rétropropagation à travers le réseau pour toutes les instances de l'ensemble de données. Voici ce qui se passe pendant une epoch :\n",
        "\n",
        "1. **Propagation Avant :**\n",
        "   - Chaque instance de l'ensemble de données est passée à travers le réseau.\n",
        "   - Le réseau fait des prédictions basées sur son état actuel (poids et biais).\n",
        "   - Les prédictions sont comparées aux objectifs réels à l'aide d'une fonction de perte pour calculer la performance du modèle.\n",
        "\n",
        "2. **Propagation Arrière (Rétropropagation) :**\n",
        "   - La perte est utilisée pour calculer le gradient de la fonction de perte par rapport à chaque poids et biais dans le réseau.\n",
        "   - Ces gradients sont utilisés pour mettre à jour les poids et les biais afin de minimiser la perte. Les mises à jour sont effectuées de manière à idéalement diminuer la perte dans les prédictions ultérieures.\n",
        "\n",
        "3. **Itération sur les Lots :**\n",
        "   - Les ensembles de données sont généralement divisés en petits ensembles appelés lots, surtout lorsqu'ils sont trop grands pour tenir en mémoire en une seule fois.\n",
        "   - Une epoch comprend un ou plusieurs lots, selon la taille de l'ensemble de données et la taille du lot. Chaque lot subit sa propre propagation avant et arrière.\n",
        "   - Le nombre de lots traités en une epoch est égal au nombre total d'échantillons divisé par la taille du lot.\n",
        "\n",
        "4. **Répétition :**\n",
        "   - Ce processus est répété pour un nombre spécifié d'epochs. À chaque epoch, le modèle apprend idéalement des motifs qui reflètent mieux les données et améliore ainsi sa précision et sa performance générale.\n",
        "\n",
        "**Choix du Nombre d'Époques :**\n",
        "Le nombre optimal d'epochs est généralement déterminé expérimentalement à travers des techniques telles que la validation croisée et le suivi de la perte de validation pendant l'entraînement. Des techniques comme l'arrêt précoce sont également utilisées, où l'entraînement peut être arrêté automatiquement lorsque la perte de validation commence à augmenter, signalant que le modèle peut commencer à surapprendre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU7d2MFpBkeV"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKy8LtOkBkeV"
      },
      "outputs": [],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km3TrzPjBkeV"
      },
      "outputs": [],
      "source": [
        "print(history.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGCAFAenBkeV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(\n",
        "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "plt.legend(loc=\"lower left\")  # extra code\n",
        "save_fig(\"keras_learning_curves_plot\")  # extra code\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrLSONdGBkeW"
      },
      "source": [
        "### Using the model to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNodQLkaBkeW"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJmiLfxjBkeW"
      },
      "outputs": [],
      "source": [
        "y_pred = y_proba.argmax(axis=-1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjV0BwFzBkeW"
      },
      "outputs": [],
      "source": [
        "np.array(class_names)[y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Va-wse8BkeW"
      },
      "outputs": [],
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHTSEJAkBkeW"
      },
      "outputs": [],
      "source": [
        "# this cell generates and saves Figure 10–12\n",
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]])\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41TWAS9kBkeX"
      },
      "source": [
        "## Construction d'un MLP de régression à l'aide de l'API séquentielle\n",
        "\n",
        "Revenons au problème du logement en Californie et abordons-le en utilisant un perceptron multicouche (MLP) construit avec Keras, en utilisant la même architecture qu'auparavant avec trois couches cachées de 50 neurones chacune. Cependant, cette fois, nous construirons le modèle en utilisant l'API séquentielle de Keras. Le processus de construction, d'entraînement, d'évaluation et d'utilisation d'un MLP de régression est similaire à celui de la classification, avec quelques différences clés :\n",
        "\n",
        "- **Couche de sortie :** Le modèle aura un seul neurone de sortie car nous ne prévoyons qu'une seule valeur, telle que le prix d'une maison. Ce neurone de sortie n'utilise pas de fonction d'activation, ce qui lui permet de produire une gamme de valeurs continues.\n",
        "\n",
        "- **Fonction de perte :** Nous utilisons l'erreur quadratique moyenne (MSE) comme fonction de perte. La MSE est standard pour les problèmes de régression et fonctionne en pénalisant le carré de l'erreur entre les prédictions et les valeurs réelles.\n",
        "\n",
        "- **Métrique :** Nous mesurons la performance en utilisant l'Erreur Quadratique Moyenne Racine (RMSE), qui fournit une métrique d'erreur sensible à l'échelle dans les mêmes unités que la valeur cible, ce qui facilite son interprétation.\n",
        "\n",
        "- **Optimiseur :** L'optimiseur Adam est utilisé, connu pour son efficacité à gérer les gradients épars et à adapter le taux d'apprentissage pendant la formation, ce qui convient à une large gamme de problèmes.\n",
        "\n",
        "- **Couche de normalisation :** Au lieu d'une couche Flatten (utilisée dans les tâches de classification pour les données d'image), nous incluons une couche de normalisation comme première couche de notre réseau. Cette couche agit comme le StandardScaler de Scikit-Learn, standardisant les caractéristiques d'entrée pour avoir une moyenne nulle et une variance unitaire. De manière cruciale, la couche de normalisation doit être \"adaptée\" aux données d'entraînement en utilisant la méthode `adapt()` avant de procéder à l'ajustement du modèle. Cela garantit que la couche échelonne correctement les caractéristiques d'entrée pendant l'entraînement et l'inférence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCzh-SQWBkeX"
      },
      "outputs": [],
      "source": [
        "# load and split the California housing dataset\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoinmBafBkeX"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
        "model = tf.keras.Sequential([\n",
        "    norm_layer,\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
        "\n",
        "norm_layer.adapt(X_train)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2TbBCzpBkeX"
      },
      "outputs": [],
      "source": [
        "rmse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mri_hnV0BkeX"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ],
      "metadata": {
        "id": "zsfwbHSD8MPy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}